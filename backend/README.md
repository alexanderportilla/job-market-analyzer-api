# üöÄ Job Market Analyzer API - Backend

**Backend completo para el an√°lisis del mercado laboral con integraci√≥n MySQL**

Una API robusta que extrae, analiza y almacena datos del mercado laboral de desarrolladores en Colombia con base de datos MySQL y funcionalidades premium.

## üèóÔ∏è Arquitectura

```
backend/
‚îú‚îÄ‚îÄ app/                    # üêç C√≥digo principal de la aplicaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Modelos SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py         # Esquemas Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ database.py        # Configuraci√≥n de base de datos
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py         # Web scraper para Computrabajo
‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py        # An√°lisis de datos
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuraci√≥n centralizada
‚îÇ   ‚îî‚îÄ‚îÄ auth.py            # Autenticaci√≥n (futuro)
‚îú‚îÄ‚îÄ scripts/               # üîß Scripts de utilidad
‚îÇ   ‚îî‚îÄ‚îÄ setup_mysql.py     # Configuraci√≥n MySQL
‚îú‚îÄ‚îÄ tests/                 # üß™ Tests unitarios
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias Python
‚îî‚îÄ‚îÄ start_with_mysql.py   # Script de inicio autom√°tico
```

## ‚ú® Caracter√≠sticas

### üóÑÔ∏è **Base de Datos MySQL**
- Configuraci√≥n autom√°tica de MySQL
- Tablas optimizadas para consultas r√°pidas
- Backup y restauraci√≥n autom√°tica
- Estad√≠sticas en tiempo real

### üîç **Web Scraping Inteligente**
- Extracci√≥n autom√°tica de Computrabajo
- Detecci√≥n de duplicados
- Manejo de errores robusto
- Respeta rate limits del servidor

### üìä **An√°lisis Avanzado**
- An√°lisis de tecnolog√≠as m√°s demandadas
- Estad√≠sticas por empresa y ubicaci√≥n
- Tendencias temporales del mercado
- Predicciones de demanda

### üîå **API REST Completa**
- Documentaci√≥n autom√°tica (Swagger)
- Endpoints para todas las funcionalidades
- Autenticaci√≥n y autorizaci√≥n
- Rate limiting y caching

## üöÄ Instalaci√≥n R√°pida

### Requisitos Previos
- Python 3.8+
- MySQL 8.0+
- Git

### Instalaci√≥n Autom√°tica

```bash
# 1. Clonar repositorio
git clone https://github.com/alexanderportilla/job-market-analyzer-api.git
cd job-market-analyzer-api/backend

# 2. Ejecutar script de inicio autom√°tico
python start_with_mysql.py
```

El script autom√°tico:
- ‚úÖ Verifica instalaci√≥n de MySQL
- ‚úÖ Configura la base de datos
- ‚úÖ Instala dependencias
- ‚úÖ Ejecuta tests de integraci√≥n
- ‚úÖ Inicia la aplicaci√≥n

### Instalaci√≥n Manual

```bash
# 1. Instalar dependencias
pip install -r requirements.txt

# 2. Configurar MySQL
python scripts/setup_mysql.py --setup

# 3. Probar integraci√≥n
python test_mysql_integration.py

# 4. Iniciar aplicaci√≥n
uvicorn app.main:app --reload --port 8000
```

## üóÑÔ∏è Configuraci√≥n MySQL

### Instalaci√≥n de MySQL

1. **Descargar MySQL Server:**
   - [MySQL Community Server](https://dev.mysql.com/downloads/mysql/)
   - Instalar con configuraci√≥n por defecto
   - Establecer contrase√±a root: `2024`

2. **Descargar MySQL Workbench:**
   - [MySQL Workbench](https://dev.mysql.com/downloads/workbench/)
   - Conectar a `localhost:3306`

### Configuraci√≥n Autom√°tica

```bash
# Configurar base de datos
python scripts/setup_mysql.py --setup

# Probar conexi√≥n
python scripts/setup_mysql.py --test

# Ver informaci√≥n de la base de datos
python test_mysql_integration.py --info
```

### Configuraci√≥n Manual

```sql
-- Conectar a MySQL
mysql -u root -p2024

-- Crear base de datos
CREATE DATABASE job_market;
USE job_market;

-- Las tablas se crean autom√°ticamente al iniciar la aplicaci√≥n
```

## üîß Uso de la API

### Endpoints Principales

#### Scraping
```bash
# Ejecutar scraper
POST /scrape/?pages=3

# Verificar estado
GET /health/
```

#### Consultas de Datos
```bash
# Obtener todas las ofertas
GET /offers/

# B√∫squeda avanzada
GET /offers/search/?q=python&location=bogota&company=techcorp

# Estad√≠sticas de tecnolog√≠as
GET /stats/technologies/
```

#### Dashboard
```bash
# Estad√≠sticas del dashboard
GET /dashboard/stats/

# Actividad reciente
GET /dashboard/recent-activity/
```

#### Analytics
```bash
# Estad√≠sticas por empresa
GET /analytics/company-stats/

# Estad√≠sticas por ubicaci√≥n
GET /analytics/location-stats/

# An√°lisis de experiencia
GET /analytics/experience-analysis/
```

### Ejemplos de Uso

#### 1. Ejecutar Scraper
```bash
curl -X POST "http://localhost:8000/scrape/?pages=2"
```

#### 2. Obtener Estad√≠sticas
```bash
curl "http://localhost:8000/dashboard/stats/"
```

#### 3. Buscar Ofertas
```bash
curl "http://localhost:8000/offers/search/?q=react&location=medellin"
```

## üìä Estructura de la Base de Datos

### Tablas Principales

#### `job_offers`
```sql
CREATE TABLE job_offers (
    id INT AUTO_INCREMENT PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    company VARCHAR(255),
    location VARCHAR(255),
    description TEXT,
    url VARCHAR(1000) UNIQUE,
    source VARCHAR(100),
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_title (title),
    INDEX idx_company (company),
    INDEX idx_location (location),
    INDEX idx_scraped_at (scraped_at)
);
```

#### `users`
```sql
CREATE TABLE users (
    id INT AUTO_INCREMENT PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    hashed_password VARCHAR(255) NOT NULL,
    full_name VARCHAR(255),
    is_active BOOLEAN DEFAULT TRUE,
    is_premium BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);
```

#### `job_alerts`
```sql
CREATE TABLE job_alerts (
    id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT,
    keywords TEXT,
    location VARCHAR(255),
    company VARCHAR(255),
    frequency VARCHAR(50) DEFAULT 'daily',
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_sent TIMESTAMP NULL,
    FOREIGN KEY (user_id) REFERENCES users(id)
);
```

#### `saved_jobs`
```sql
CREATE TABLE saved_jobs (
    id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT,
    job_offer_id INT,
    saved_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    notes TEXT,
    FOREIGN KEY (user_id) REFERENCES users(id),
    FOREIGN KEY (job_offer_id) REFERENCES job_offers(id)
);
```

## üîç Funcionalidades del Scraper

### Caracter√≠sticas del Scraper
- **Extracci√≥n autom√°tica** de Computrabajo
- **Detecci√≥n de duplicados** por URL
- **Manejo de errores** robusto
- **Rate limiting** para respetar el servidor
- **Logging detallado** de operaciones
- **Commit por p√°gina** para evitar p√©rdida de datos

### Configuraci√≥n del Scraper
```python
# En app/config.py
BASE_URL = "https://www.computrabajo.com.co/ofertas-de-trabajo/?q=python"
MAX_PAGES = 10
REQUEST_TIMEOUT = 15
DELAY_BETWEEN_REQUESTS = 1.0
```

### Uso del Scraper
```python
from app.scraper import scrape_job_offers
from app.database import get_db

# Ejecutar scraper
db = next(get_db())
result = scrape_job_offers(db, pages=3)
print(result)
```

## üìà An√°lisis de Datos

### Funciones de An√°lisis
```python
from app.analyzer import analyze_technology_demand

# Analizar demanda de tecnolog√≠as
tech_stats = analyze_technology_demand(db)
for tech in tech_stats:
    print(f"{tech.technology}: {tech.percentage}%")
```

### Estad√≠sticas Disponibles
- **Tecnolog√≠as m√°s demandadas**
- **Empresas m√°s activas**
- **Ubicaciones con m√°s oportunidades**
- **Tendencias temporales**
- **An√°lisis de experiencia requerida**

## üß™ Testing

### Tests de Integraci√≥n
```bash
# Ejecutar todos los tests
python test_mysql_integration.py

# Test espec√≠fico
python test_mysql_integration.py --connection
python test_mysql_integration.py --insert
python test_mysql_integration.py --scraping
```

### Tests Unitarios
```bash
# Ejecutar tests unitarios
pytest tests/

# Con coverage
pytest --cov=app tests/
```

## üê≥ Docker

### Construir Imagen
```bash
docker build -t job-market-analyzer-backend .
```

### Ejecutar con Docker Compose
```bash
docker-compose up --build
```

### Variables de Entorno
```bash
# .env
DATABASE_URL=mysql+mysqlconnector://root:2024@mysql:3306/job_market
API_HOST=0.0.0.0
API_PORT=8000
ENVIRONMENT=production
```

## üîß Configuraci√≥n Avanzada

### Variables de Entorno
```bash
# Base de datos
DATABASE_URL=mysql+mysqlconnector://root:2024@localhost:3306/job_market

# API
API_HOST=localhost
API_PORT=8000
ENVIRONMENT=development

# Scraping
BASE_URL=https://www.computrabajo.com.co/ofertas-de-trabajo/?q=python
MAX_PAGES=10
REQUEST_TIMEOUT=15
DELAY_BETWEEN_REQUESTS=1.0
```

### Logging
```python
# Configurar logging
import logging
logging.basicConfig(level=logging.INFO)
```

### Performance
```python
# Optimizar consultas
from sqlalchemy import create_engine
engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_recycle=300)
```

## üöÄ Deployment

### Producci√≥n
```bash
# Instalar dependencias de producci√≥n
pip install -r requirements.txt

# Configurar base de datos
python scripts/setup_mysql.py --setup

# Ejecutar con Gunicorn
gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

### Cloud Platforms
- **Heroku**: Configurar add-on MySQL
- **Railway**: Deploy autom√°tico
- **DigitalOcean**: Droplet con MySQL
- **AWS**: RDS MySQL + EC2

## üìö Documentaci√≥n API

### Swagger UI
- **URL**: http://localhost:8000/docs
- **Documentaci√≥n interactiva**
- **Pruebas de endpoints**

### ReDoc
- **URL**: http://localhost:8000/redoc
- **Documentaci√≥n alternativa**

## ü§ù Contribuci√≥n

1. Fork el proyecto
2. Crear rama feature (`git checkout -b feature/AmazingFeature`)
3. Commit cambios (`git commit -m 'Add AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver el archivo `LICENSE` para m√°s detalles.

## üÜò Soporte

- **Issues**: [GitHub Issues](https://github.com/alexanderportilla/job-market-analyzer-api/issues)
- **Documentaci√≥n**: [Wiki del proyecto](https://github.com/alexanderportilla/job-market-analyzer-api/wiki)
- **Email**: alexander.portilla@example.com

---

**Desarrollado con ‚ù§Ô∏è por Alexander Portilla**

*Transformando el an√°lisis del mercado laboral en una experiencia premium* 